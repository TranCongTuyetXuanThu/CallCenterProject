{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-16T13:00:48.785990Z","iopub.execute_input":"2023-10-16T13:00:48.786385Z","iopub.status.idle":"2023-10-16T13:00:48.799941Z","shell.execute_reply.started":"2023-10-16T13:00:48.786355Z","shell.execute_reply":"2023-10-16T13:00:48.798590Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/kaggle/input/iemocap-trainingdata/data_test.pt\n/kaggle/input/iemocap-trainingdata/label_test.pt\n/kaggle/input/iemocap-trainingdata/label_train.pt\n/kaggle/input/iemocap-trainingdata/data_train.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport torch.optim as optim\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T13:06:59.623219Z","iopub.execute_input":"2023-10-16T13:06:59.624300Z","iopub.status.idle":"2023-10-16T13:06:59.629506Z","shell.execute_reply.started":"2023-10-16T13:06:59.624259Z","shell.execute_reply":"2023-10-16T13:06:59.628284Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Đường dẫn đến các file\ndata_train_path = \"/kaggle/input/iemocap-trainingdata/data_train.pt\"\nlabel_train_path = \"/kaggle/input/iemocap-trainingdata/label_train.pt\"\ndata_test_path = \"/kaggle/input/iemocap-trainingdata/data_test.pt\"\nlabel_test_path = \"/kaggle/input/iemocap-trainingdata/label_test.pt\"\n\n# Load dữ liệu từ các file\ndata_train = torch.load(data_train_path)\nlabel_train = torch.load(label_train_path)\ndata_test = torch.load(data_test_path)\nlabel_test = torch.load(label_test_path)\n\n# Chuẩn bị DataLoader cho dữ liệu huấn luyện và kiểm tra\nbatch_size = 32  # Thay đổi batch size theo nhu cầu của bạn\n\n# Chuẩn bị DataLoader cho dữ liệu huấn luyện và kiểm tra\ntrain_dataset = TensorDataset(data_train, label_train)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = TensorDataset(data_test, label_test)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define a custom dataset\nclass CustomDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n        return sample\n\n# Create DataLoader for training and testing using the custom dataset\ntrain_dataset_custom = CustomDataset(data_train, label_train)\ntrain_loader_custom = DataLoader(train_dataset_custom, batch_size=batch_size, shuffle=True)\n\ntest_dataset_custom = CustomDataset(data_test, label_test)\ntest_loader_custom = DataLoader(test_dataset_custom, batch_size=batch_size, shuffle=False)\n\n# Rest of your model and training code goes here\n# Ensure you use the DataLoader to load data and labels during training and evaluation\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T13:07:02.161197Z","iopub.execute_input":"2023-10-16T13:07:02.162430Z","iopub.status.idle":"2023-10-16T13:07:03.813425Z","shell.execute_reply.started":"2023-10-16T13:07:02.162391Z","shell.execute_reply":"2023-10-16T13:07:03.812282Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# class ComplexNN(nn.Module):\n#     def __init__(self, cnn_input_channels, cnn_output_channels, cnn_kernel_size, cnn_stride, \n#                  cnn_pool_kernel_size, cnn_pool_stride, hidden_size, output_size):\n#         super(ComplexNN, self).__init__()\n        \n#         self.cnn = nn.Conv1d(cnn_input_channels, cnn_output_channels, cnn_kernel_size, stride=cnn_stride)\n#         self.pool = nn.MaxPool1d(cnn_pool_kernel_size, stride=cnn_pool_stride)\n        \n#         self.input_size = cnn_output_channels * ((1500 - cnn_kernel_size) // cnn_pool_kernel_size + 1)\n#         self.hidden = nn.Linear(self.input_size, hidden_size)\n#         self.relu = nn.ReLU()\n        \n#         self.output = nn.Linear(hidden_size, output_size)\n\n#     def forward(self, x):\n#         x = self.cnn(x)\n#         x = self.pool(x)\n#         x = x.view(x.size(0), -1)\n#         x = self.hidden(x)\n#         x = self.relu(x)\n#         x = self.output(x)\n#         x = F.softmax(x, dim=1)\n#         return x\n\n# # Define model parameters\n# cnn_input_channels = 40\n# cnn_output_channels = 16\n# cnn_kernel_size = 5\n# cnn_stride = 1\n# cnn_pool_kernel_size = 2\n# cnn_pool_stride = 2\n# hidden_size = 128\n# output_size = 3\n\n# # Create an instance of the ComplexNN model\n# model = ComplexNN(cnn_input_channels, cnn_output_channels, cnn_kernel_size, cnn_stride,\n#                    cnn_pool_kernel_size, cnn_pool_stride, hidden_size, output_size)\n\n# # Print the model architecture\n# print(model)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T13:07:45.812880Z","iopub.execute_input":"2023-10-16T13:07:45.813293Z","iopub.status.idle":"2023-10-16T13:07:45.837690Z","shell.execute_reply.started":"2023-10-16T13:07:45.813237Z","shell.execute_reply":"2023-10-16T13:07:45.836480Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"ComplexNN(\n  (cnn): Conv1d(40, 16, kernel_size=(5,), stride=(1,))\n  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (hidden): Linear(in_features=11968, out_features=128, bias=True)\n  (relu): ReLU()\n  (output): Linear(in_features=128, out_features=3, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ComplexNN(nn.Module):\n    def __init__(self, cnn_input_channels, cnn_output_channels, cnn_kernel_size, cnn_stride, \n                 cnn_pool_kernel_size, cnn_pool_stride, hidden_size, output_size):\n        super(ComplexNN, self).__init__()\n        \n        self.cnn = nn.Conv1d(cnn_input_channels, cnn_output_channels, cnn_kernel_size, stride=cnn_stride)\n        self.pool = nn.MaxPool1d(cnn_pool_kernel_size, stride=cnn_pool_stride)\n        \n        # Calculate the input size for the linear layer based on the new input dimensions\n        # Adjust for your specific input size (40, 1500)\n        self.input_size = cnn_output_channels * ((1500 - cnn_kernel_size) // cnn_stride + 1) // cnn_pool_kernel_size\n        self.hidden = nn.Linear(self.input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.output = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.hidden(x)\n        x = self.relu(x)\n        x = self.output(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n# Define updated model parameters for the new input dimensions (40, 1500)\ncnn_input_channels = 40\ncnn_output_channels = 16\ncnn_kernel_size = 5\ncnn_stride = 1\ncnn_pool_kernel_size = 2\ncnn_pool_stride = 2\nhidden_size = 128\noutput_size = 3\n\n# Create an instance of the updated ComplexNN model\nupdated_model = ComplexNN(cnn_input_channels, cnn_output_channels, cnn_kernel_size, cnn_stride,\n                          cnn_pool_kernel_size, cnn_pool_stride, hidden_size, output_size)\n\n# Print the updated model architecture\nprint(updated_model)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T13:33:47.220647Z","iopub.execute_input":"2023-10-16T13:33:47.221042Z","iopub.status.idle":"2023-10-16T13:33:47.247171Z","shell.execute_reply.started":"2023-10-16T13:33:47.221014Z","shell.execute_reply":"2023-10-16T13:33:47.246037Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"ComplexNN(\n  (cnn): Conv1d(40, 16, kernel_size=(5,), stride=(1,))\n  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (hidden): Linear(in_features=11968, out_features=128, bias=True)\n  (relu): ReLU()\n  (output): Linear(in_features=128, out_features=3, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.optim as optim\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Move the model to the appropriate device\nmodel.to(device)\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)  # Adjust the learning rate as needed\n\n# Training loop\nnum_epochs = 100  # Adjust the number of epochs as needed\n\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    running_loss = 0.0\n    \n    for batch_data, batch_labels in train_loader:\n        # Move the data to the appropriate device\n        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(batch_data)\n        loss = criterion(outputs, torch.argmax(batch_labels, dim=1))\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Update running loss\n        running_loss += loss.item()\n    \n    # Print the average loss for the epoch\n    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n\nprint('Finished Training')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T13:50:16.362764Z","iopub.execute_input":"2023-10-16T13:50:16.363164Z","iopub.status.idle":"2023-10-16T13:51:50.497157Z","shell.execute_reply.started":"2023-10-16T13:50:16.363134Z","shell.execute_reply":"2023-10-16T13:51:50.496053Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Using device: cuda\nEpoch 1, Loss: 1.079695087892038\nEpoch 2, Loss: 1.0785140578078214\nEpoch 3, Loss: 1.0791045752151933\nEpoch 4, Loss: 1.079695087261301\nEpoch 5, Loss: 1.0779235482846619\nEpoch 6, Loss: 1.0791045739537193\nEpoch 7, Loss: 1.0796950897842488\nEpoch 8, Loss: 1.0791045755305617\nEpoch 9, Loss: 1.0791045739537193\nEpoch 10, Loss: 1.077333032769501\nEpoch 11, Loss: 1.0796950904149858\nEpoch 12, Loss: 1.0779235467078194\nEpoch 13, Loss: 1.0785140612768749\nEpoch 14, Loss: 1.0791045755305617\nEpoch 15, Loss: 1.0791045739537193\nEpoch 16, Loss: 1.0791045752151933\nEpoch 17, Loss: 1.0802856027764618\nEpoch 18, Loss: 1.0791045752151933\nEpoch 19, Loss: 1.078514061907612\nEpoch 20, Loss: 1.0802856037225672\nEpoch 21, Loss: 1.0791045739537193\nEpoch 22, Loss: 1.0791045745844563\nEpoch 23, Loss: 1.0785140606461379\nEpoch 24, Loss: 1.0802856024610934\nEpoch 25, Loss: 1.0796950888381434\nEpoch 26, Loss: 1.079104574269088\nEpoch 27, Loss: 1.077923545130977\nEpoch 28, Loss: 1.07792354450024\nEpoch 29, Loss: 1.0791045764766674\nEpoch 30, Loss: 1.0791045745844563\nEpoch 31, Loss: 1.0785140622229803\nEpoch 32, Loss: 1.0785140593846638\nEpoch 33, Loss: 1.077923545130977\nEpoch 34, Loss: 1.0785140600154008\nEpoch 35, Loss: 1.0785140612768749\nEpoch 36, Loss: 1.0802856024610934\nEpoch 37, Loss: 1.0796950900996174\nEpoch 38, Loss: 1.0785140615922433\nEpoch 39, Loss: 1.0785140609615063\nEpoch 40, Loss: 1.0785140600154008\nEpoch 41, Loss: 1.0791045755305617\nEpoch 42, Loss: 1.0796950891535118\nEpoch 43, Loss: 1.077923545130977\nEpoch 44, Loss: 1.0785140609615063\nEpoch 45, Loss: 1.077923545761714\nEpoch 46, Loss: 1.0785140606461379\nEpoch 47, Loss: 1.0785140590692954\nEpoch 48, Loss: 1.0785140612768749\nEpoch 49, Loss: 1.0791045739537193\nEpoch 50, Loss: 1.0791045730076139\nEpoch 51, Loss: 1.0779235460770824\nEpoch 52, Loss: 1.0785140603307695\nEpoch 53, Loss: 1.0785140609615063\nEpoch 54, Loss: 1.0796950894688804\nEpoch 55, Loss: 1.0785140597000324\nEpoch 56, Loss: 1.0785140590692954\nEpoch 57, Loss: 1.0791045764766674\nEpoch 58, Loss: 1.0791045752151933\nEpoch 59, Loss: 1.079104574899825\nEpoch 60, Loss: 1.0791045752151933\nEpoch 61, Loss: 1.0791045736383509\nEpoch 62, Loss: 1.0785140609615063\nEpoch 63, Loss: 1.0796950882074063\nEpoch 64, Loss: 1.079104574899825\nEpoch 65, Loss: 1.0779235460770824\nEpoch 66, Loss: 1.0785140600154008\nEpoch 67, Loss: 1.0796950882074063\nEpoch 68, Loss: 1.0796950863151955\nEpoch 69, Loss: 1.0802856034071988\nEpoch 70, Loss: 1.076742515362129\nEpoch 71, Loss: 1.079104574899825\nEpoch 72, Loss: 1.0785140609615063\nEpoch 73, Loss: 1.0785140593846638\nEpoch 74, Loss: 1.0791045733229825\nEpoch 75, Loss: 1.0796950891535118\nEpoch 76, Loss: 1.0791045745844563\nEpoch 77, Loss: 1.0785140593846638\nEpoch 78, Loss: 1.077333032769501\nEpoch 79, Loss: 1.0779235476539248\nEpoch 80, Loss: 1.0796950891535118\nEpoch 81, Loss: 1.0791045726922455\nEpoch 82, Loss: 1.0791045767920358\nEpoch 83, Loss: 1.0791045761612987\nEpoch 84, Loss: 1.0796950907303542\nEpoch 85, Loss: 1.0791045730076139\nEpoch 86, Loss: 1.0791045733229825\nEpoch 87, Loss: 1.0785140612768749\nEpoch 88, Loss: 1.0791045764766674\nEpoch 89, Loss: 1.079695087892038\nEpoch 90, Loss: 1.079695087892038\nEpoch 91, Loss: 1.078514062538349\nEpoch 92, Loss: 1.079104574899825\nEpoch 93, Loss: 1.0779235463924508\nEpoch 94, Loss: 1.079104574899825\nEpoch 95, Loss: 1.0779235460770824\nEpoch 96, Loss: 1.077333032138764\nEpoch 97, Loss: 1.079695087261301\nEpoch 98, Loss: 1.0779235435541346\nEpoch 99, Loss: 1.0796950891535118\nEpoch 100, Loss: 1.0791045767920358\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from sklearn.metrics import f1_score, accuracy_score\n\n# # Check for GPU availability\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(\"Using device:\", device)\n\n# # Move the model to the appropriate device\n# model.to(device)\n\n# # Loss function and optimizer\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust the learning rate as needed\n\n# # Training loop\n# num_epochs = 100  # Adjust the number of epochs as needed\n\n# for epoch in range(num_epochs):\n#     model.train()  # Set the model to training mode\n#     running_loss = 0.0\n#     total_predictions = []\n#     true_labels = []\n    \n#     for batch_data, batch_labels in train_loader:\n#         # Move the data to the appropriate device\n#         batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n        \n#         # Zero the parameter gradients\n#         optimizer.zero_grad()\n        \n#         # Forward pass\n#         outputs = model(batch_data)\n#         loss = criterion(outputs, torch.argmax(batch_labels, dim=1))\n        \n#         # Backward pass and optimize\n#         loss.backward()\n#         optimizer.step()\n        \n#         # Update running loss\n#         running_loss += loss.item()\n        \n#          # Update true labels and predictions for F1-score and accuracy\n#         total_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n#         true_labels.extend(torch.argmax(batch_labels, dim=1).cpu().numpy())\n    \n#     # Calculate F1-score and accuracy\n#     f1 = f1_score(true_labels, total_predictions, average='weighted')\n#     acc = accuracy_score(true_labels, total_predictions)\n    \n#     # Print the average loss, F1-score, and accuracy for the epoch\n#     print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}, F1-score: {f1}, Accuracy: {acc}\")\n\n# print('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2023-10-16T13:40:32.022124Z","iopub.execute_input":"2023-10-16T13:40:32.023305Z","iopub.status.idle":"2023-10-16T13:42:09.831308Z","shell.execute_reply.started":"2023-10-16T13:40:32.023259Z","shell.execute_reply":"2023-10-16T13:42:09.830180Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Using device: cuda\nEpoch 1, Loss: 1.0796950900996174, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 2, Loss: 1.0796950875766693, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 3, Loss: 1.077923545130977, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 4, Loss: 1.0779235476539248, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 5, Loss: 1.079104574899825, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 6, Loss: 1.0785140600154008, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 7, Loss: 1.0779235479692932, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 8, Loss: 1.0785140600154008, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 9, Loss: 1.0791045752151933, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 10, Loss: 1.0791045764766674, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 11, Loss: 1.0791045739537193, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 12, Loss: 1.0773330324541324, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 13, Loss: 1.077923545761714, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 14, Loss: 1.0791045764766674, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 15, Loss: 1.0791045745844563, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 16, Loss: 1.0785140615922433, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 17, Loss: 1.0785140600154008, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 18, Loss: 1.0791045739537193, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 19, Loss: 1.0785140603307695, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 20, Loss: 1.0785140597000324, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 21, Loss: 1.0785140609615063, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 22, Loss: 1.0785140609615063, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 23, Loss: 1.0785140590692954, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 24, Loss: 1.0785140612768749, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 25, Loss: 1.0796950907303542, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 26, Loss: 1.0791045755305617, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 27, Loss: 1.0791045758459303, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 28, Loss: 1.0785140609615063, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 29, Loss: 1.0785140593846638, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 30, Loss: 1.077923545761714, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 31, Loss: 1.0785140603307695, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 32, Loss: 1.0785140590692954, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 33, Loss: 1.080285600884251, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 34, Loss: 1.0785140609615063, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 35, Loss: 1.0791045771074044, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 36, Loss: 1.0791045774227728, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 37, Loss: 1.0796950882074063, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 38, Loss: 1.079695087892038, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 39, Loss: 1.0785140612768749, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 40, Loss: 1.0791045758459303, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 41, Loss: 1.0802856052994096, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 42, Loss: 1.0785140600154008, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 43, Loss: 1.0802856034071988, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 44, Loss: 1.079104574269088, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 45, Loss: 1.0791045752151933, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 46, Loss: 1.0791045733229825, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 47, Loss: 1.0791045730076139, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 48, Loss: 1.078514062538349, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 49, Loss: 1.0785140593846638, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 50, Loss: 1.0785140606461379, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 51, Loss: 1.0796950923071966, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 52, Loss: 1.0791045758459303, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 53, Loss: 1.0791045745844563, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 54, Loss: 1.0785140587539268, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 55, Loss: 1.0802856030918302, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 56, Loss: 1.0791045777381412, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 57, Loss: 1.0791045755305617, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 58, Loss: 1.0779235492307673, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 59, Loss: 1.0791045739537193, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 60, Loss: 1.0791045755305617, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 61, Loss: 1.0802856037225672, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 62, Loss: 1.078514061907612, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 63, Loss: 1.0785140587539268, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 64, Loss: 1.0791045764766674, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 65, Loss: 1.0791045767920358, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 66, Loss: 1.0791045761612987, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 67, Loss: 1.0802856030918302, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 68, Loss: 1.0791045752151933, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 69, Loss: 1.0791045758459303, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 70, Loss: 1.0785140603307695, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 71, Loss: 1.0791045767920358, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 72, Loss: 1.0779235479692932, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 73, Loss: 1.0785140606461379, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 74, Loss: 1.0785140603307695, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 75, Loss: 1.0785140603307695, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 76, Loss: 1.0802856021457248, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 77, Loss: 1.079695087261301, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 78, Loss: 1.077923545761714, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 79, Loss: 1.0791045752151933, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 80, Loss: 1.0785140597000324, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 81, Loss: 1.0785140622229803, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 82, Loss: 1.078514057492453, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 83, Loss: 1.0791045739537193, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 84, Loss: 1.0785140637998227, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 85, Loss: 1.079104574269088, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 86, Loss: 1.0796950885227747, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 87, Loss: 1.0791045752151933, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 88, Loss: 1.0791045761612987, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 89, Loss: 1.0791045758459303, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 90, Loss: 1.079104574269088, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 91, Loss: 1.0779235467078194, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 92, Loss: 1.0785140600154008, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 93, Loss: 1.0773330324541324, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 94, Loss: 1.077923545130977, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 95, Loss: 1.0785140612768749, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 96, Loss: 1.0808761154533064, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 97, Loss: 1.0791045723768768, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 98, Loss: 1.0785140600154008, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 99, Loss: 1.079695087261301, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 100, Loss: 1.0808761163994118, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Set the model to evaluation mode\n# model.eval()\n\n# # Lists to store predictions and true labels for evaluation\n# eval_predictions = []\n# eval_true_labels = []\n\n# # Iterate over the test dataset\n# for eval_batch_data, eval_batch_labels in test_loader:\n#     # Move the data to the appropriate device\n#     eval_batch_data, eval_batch_labels = eval_batch_data.to(device), eval_batch_labels.to(device)\n    \n#     # Perform inference\n#     with torch.no_grad():\n#         eval_outputs = model(eval_batch_data)\n    \n#     # Append the predictions and true labels\n#     eval_predictions.extend(torch.argmax(eval_outputs, dim=1).cpu().numpy())\n#     eval_true_labels.extend(torch.argmax(eval_batch_labels, dim=1).cpu().numpy())\n\n# # Calculate F1-score and accuracy on the test set\n# eval_f1 = f1_score(eval_true_labels, eval_predictions, average='weighted')\n# eval_acc = accuracy_score(eval_true_labels, eval_predictions)\n\n# # Print the evaluation metrics\n# print(\"Evaluation F1-score:\", eval_f1)\n# print(\"Evaluation Accuracy:\", eval_acc)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T11:44:01.081861Z","iopub.execute_input":"2023-10-16T11:44:01.082725Z","iopub.status.idle":"2023-10-16T11:44:01.708500Z","shell.execute_reply.started":"2023-10-16T11:44:01.082684Z","shell.execute_reply":"2023-10-16T11:44:01.707324Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Evaluation F1-score: 0.408752857653176\nEvaluation Accuracy: 0.4893758300132802\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Move the model to the appropriate device\nmodel = model.to(device)\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust the learning rate as needed\n\n# Initialize variables for early stopping and best model\nearly_stopping_counter = 0\nbest_test_loss = float('inf')\nbest_model_weights = None\nloss_train = []\nloss_test = []\nf1_train = []\nf1_test = []\n\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    running_loss = 0.0\n    total_predictions = []\n    true_labels = []\n    \n    for batch_data, batch_labels in train_loader:\n        # Move the data to the appropriate device\n        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(batch_data)\n        loss = criterion(outputs, torch.argmax(batch_labels, dim=1))\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Update running loss\n        running_loss += loss.item()\n        \n        # Update true labels and predictions for F1-score and accuracy\n        total_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n        true_labels.extend(torch.argmax(batch_labels, dim=1).cpu().numpy())\n    \n    # Calculate F1-score and accuracy\n    f1 = f1_score(true_labels, total_predictions, average='weighted')\n    acc = accuracy_score(true_labels, total_predictions)\n    \n    # Print the average loss, F1-score, and accuracy for the epoch\n    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}, F1-score: {f1}, Accuracy: {acc}\")\n\n    # Check if the test loss has improved for early stopping\n    if running_loss < best_test_loss:\n        best_test_loss = running_loss\n        early_stopping_counter = 0\n        best_model_weights = model.state_dict()\n    else:\n        early_stopping_counter += 1\n\n    # Check for early stopping\n    if early_stopping_counter >= 5:\n        print(\"Early stopping triggered. No improvement for 5 consecutive epochs.\")\n        break\n\n# Load the best model weights before saving\nif best_model_weights is not None:\n    model.load_state_dict(best_model_weights)\n\n# Save the weights of the best model\ntorch.save(model.state_dict(), 'best_model_weights.pth')\nprint('Best model weights saved as best_model_weights.pth')\n\n# Save the final model weights\ntorch.save(model.state_dict(), 'final_model_weights.pth')\nprint('Final model weights saved as final_model_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2023-10-16T13:56:42.920024Z","iopub.execute_input":"2023-10-16T13:56:42.920509Z","iopub.status.idle":"2023-10-16T13:56:59.697629Z","shell.execute_reply.started":"2023-10-16T13:56:42.920473Z","shell.execute_reply":"2023-10-16T13:56:59.696580Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Using device: cuda\nEpoch 1, Loss: 1.0796950882074063, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 2, Loss: 1.0785140631690857, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 3, Loss: 1.077923545130977, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 4, Loss: 1.0785140603307695, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 5, Loss: 1.0791045739537193, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 6, Loss: 1.0791045752151933, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 7, Loss: 1.0773330330848694, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 8, Loss: 1.0791045739537193, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 9, Loss: 1.0796950897842488, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 10, Loss: 1.0791045761612987, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 11, Loss: 1.0791045764766674, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 12, Loss: 1.0773330324541324, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 13, Loss: 1.0796950888381434, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 14, Loss: 1.0785140584385584, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 15, Loss: 1.0779235463924508, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 16, Loss: 1.0785140603307695, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEpoch 17, Loss: 1.0796950875766693, F1-score: 0.303258001841215, Accuracy: 0.4725219990038187\nEarly stopping triggered. No improvement for 5 consecutive epochs.\nBest model weights saved as best_model_weights.pth\nFinal model weights saved as final_model_weights.pth\n","output_type":"stream"}]}]}