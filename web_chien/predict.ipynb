{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlOg5ZAGcuJr",
        "outputId": "a1c2a4eb-2bb8-4c14-89df-78a553a1a23c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from torch import nn\n",
        "import torch, torchaudio, statistics\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import detect_nonsilent"
      ],
      "metadata": {
        "id": "McukFf_5aphR"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model flow\n",
        "class define_model(nn.Module):\n",
        "\n",
        "    # Define layers\n",
        "    def __init__(self, num_emotions):\n",
        "        super().__init__()\n",
        "\n",
        "        transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=40, #####################\n",
        "            nhead=4,\n",
        "            dim_feedforward=512,\n",
        "            dropout=0.4,\n",
        "            activation='relu'\n",
        "        )\n",
        "        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
        "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n",
        "\n",
        "        #maxpool: reshape (width, height)\n",
        "        #conv: reshape (channel)\n",
        "        conv2d_layer = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),#######\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=32,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(32),#######\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(64),#######\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
        "            nn.Dropout(p=0.3),\n",
        "        )\n",
        "        self.conv2Dblock1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),#######\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=32,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(32),#######\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(64),#######\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
        "            nn.Dropout(p=0.3),\n",
        "        )\n",
        "        self.conv2Dblock2 = conv2d_layer\n",
        "\n",
        "        self.fc1_layer = nn.Linear(960*2+40, 980)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.fc2_layer = nn.Linear(980, num_emotions)\n",
        "        self.softmax_out = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv2d_embedding1 = self.conv2Dblock1(x)\n",
        "        conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim = 1)\n",
        "\n",
        "        conv2d_embedding2 = self.conv2Dblock2(x)\n",
        "        conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim = 1)\n",
        "\n",
        "        x_maxpool = self.transformer_maxpool(x)\n",
        "        x_maxpool_reduced = torch.squeeze(x_maxpool,1) ############\n",
        "        x = x_maxpool_reduced.permute(2,0,1) ###########\n",
        "        transformer_output = self.transformer_encoder(x)\n",
        "        transformer_embedding = torch.mean(transformer_output, dim = 0)\n",
        "\n",
        "        complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2, transformer_embedding], dim = 1)\n",
        "        fc1 = self.fc1_layer(complete_embedding)\n",
        "        ac1 = self.act1(fc1)\n",
        "        output_logits = self.fc2_layer(ac1)\n",
        "        output_softmax = self.softmax_out(output_logits)\n",
        "        return output_logits, output_softmax"
      ],
      "metadata": {
        "id": "NphIprrdafyX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class preprocess_real_data():\n",
        "    def __init__(self,file):\n",
        "      self.file = file\n",
        "    def mp4_to_wav(self, file):\n",
        "        # Load the MP4 file\n",
        "        # video = VideoFileClip(file)\n",
        "        # # Extract the audio from the video\n",
        "        # audio = video.audio\n",
        "        # # Export the audio as a WAV file\n",
        "        # file = file[:-4]+'.wav'\n",
        "        # audio.write_audiofile(file)\n",
        "        output_file = file.split('.')[0] + \".wav\"\n",
        "        subprocess.call(['ffmpeg', '-i', file, output_file])\n",
        "        return output_file\n",
        "    def remove_noise(self, file):\n",
        "        # Detect non-silent parts of the audio\n",
        "        sound_file = AudioSegment.from_wav(file)\n",
        "        non_sil_times = detect_nonsilent(sound_file, min_silence_len=400, silence_thresh=sound_file.dBFS * 0.65)\n",
        "\n",
        "        # Concatenate the non-silent parts of the audio\n",
        "        if len(non_sil_times) > 0:\n",
        "            non_sil_times_concat = [non_sil_times[0]]\n",
        "            if len(non_sil_times) > 1:\n",
        "                for t in non_sil_times[1:]:\n",
        "                    if t[0] - non_sil_times_concat[-1][1] < 100:\n",
        "                        non_sil_times_concat[-1] = (non_sil_times_concat[-1][0], t[1])\n",
        "                    else:\n",
        "                        non_sil_times_concat.append(t)\n",
        "            new_audio = sound_file[non_sil_times_concat[0][0]:non_sil_times_concat[0][1]]\n",
        "            for t in non_sil_times_concat[1:]:\n",
        "                new_audio += sound_file[t[0]:t[1]]\n",
        "        else:\n",
        "            new_audio = sound_file\n",
        "\n",
        "        # Export the new audio file\n",
        "        file_name = file[:-4]+'_denoised.wav'\n",
        "        new_audio.export(file[:-4]+'_denoised.wav', format=\"wav\")\n",
        "        return file_name\n",
        "    def resize(self, file):\n",
        "        tensor = torch.load(file)\n",
        "        tensor1 = tensor[0,:,:]\n",
        "        # tensor2 = tensor[1,:,:]\n",
        "        n = tensor1.shape[-1]//500 + 1\n",
        "        sample1 = torch.zeros((40,n * 500))\n",
        "        sample1[:,:tensor1.shape[-1]] = tensor1\n",
        "        sample1 = torch.transpose(torch.transpose(sample1.reshape((40,500,n)),dim0=0, dim1=2),dim0=1, dim1=2).reshape((n,1,40,500))\n",
        "        return sample1\n",
        "    def save_tensor_file(self, file):\n",
        "        waveform, sample_rate = torchaudio.load(file)\n",
        "        transform = torchaudio.transforms.MFCC(sample_rate=sample_rate)\n",
        "        mfcc = transform(waveform)\n",
        "        file_name = file[:-4]+'.pt'\n",
        "        torch.save(mfcc,file_name)\n",
        "        return file_name\n",
        "    def complete_preprocessing(self, file):\n",
        "      if file[-4:] != '.wav':\n",
        "          file = self.mp4_to_wav(file)\n",
        "      file = self.remove_noise(file)\n",
        "      file = self.save_tensor_file(file)\n",
        "      tensor = self.resize(file)\n",
        "      return file, tensor\n",
        ""
      ],
      "metadata": {
        "id": "J8dYFNxIZ4ky"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path):\n",
        "    model = define_model(3)\n",
        "    model.eval()\n",
        "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    # model.load_state_dict(torch.load(model_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "DRenZW-aaSf2"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_1_sample(file_sample, model, preprocesser):\n",
        "    file_sample, tensor = preprocesser.complete_preprocessing(file_sample)\n",
        "    output_logit, output_softmax = model(tensor)\n",
        "    output_softmax = torch.argmax(output_softmax, dim=1)\n",
        "    final_output = max(set(output_softmax.tolist()), key=output_softmax.tolist().count)\n",
        "    emotion_dict = {0: 'positive',\n",
        "                    1: 'neutral',\n",
        "                    2: 'negative'}\n",
        "    label = emotion_dict[final_output]\n",
        "    return final_output, label\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "onMFksqDaXSk"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('model95.pt')\n",
        "preprocess = preprocess_real_data('ok')\n",
        "final_output, label = predict_1_sample('2.m4a', model, preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeCOE5N_cIxx",
        "outputId": "30b51a4f-2620-4426-b63b-6b4f4c6b1e34"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "35fDpESJqMzN",
        "outputId": "803cd4c6-3f9b-4d75-ebac-6b5cabfd4049"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neutral'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4t8PMCMqOdC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}