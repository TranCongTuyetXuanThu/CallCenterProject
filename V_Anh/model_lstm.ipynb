{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Think\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "import torch\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from torch import Tensor, nn, optim\n",
    "from torchvggish import vggish\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvggish.vggish_input import waveform_to_examples\n",
    "from tqdm.auto import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "2023-10-26 11:56:34,318 - root - INFO - Train samples: 6023\n",
      "2023-10-26 11:56:34,319 - root - INFO - Test samples: 1506\n",
      "2023-10-26 11:56:34,319 - root - INFO - Saved to IEMOCAP_preprocessed\n",
      "2023-10-26 11:56:34,320 - root - INFO - Preprocessing finished successfully\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "data_root = \"D:\\LSTM\\IEMOCAP_full_release\"\n",
    "ignore_length = 0\n",
    "\n",
    "session_id = list(range(1, 6))\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "iemocap2label = {\n",
    "    \"neu\": 0,\n",
    "    \"fru\": 0,\n",
    "    \"exc\": 1,\n",
    "    \"hap\": 1,\n",
    "    \"sur\": 1,\n",
    "    \"fea\": 2,\n",
    "    \"ang\": 2,\n",
    "    \"sad\": 2,\n",
    "    \"dis\": 2,\n",
    "}\n",
    "iemocap2label.update({\"exc\": 1})\n",
    "\n",
    "for sess_id in tqdm(session_id):\n",
    "    sess_path = os.path.join(data_root, \"Session{}\".format(sess_id))\n",
    "    sess_autio_root = os.path.join(sess_path, \"sentences/wav\")\n",
    "    sess_label_root = os.path.join(sess_path, \"dialog/EmoEvaluation\")\n",
    "    label_paths = glob.glob(os.path.join(sess_label_root, \"*.txt\"))\n",
    "    for l_path in label_paths:\n",
    "        with open(l_path, \"r\") as f:\n",
    "            label = f.read().split(\"\\n\")\n",
    "            for l in label:\n",
    "                if str(l).startswith(\"[\"):\n",
    "                    data = l.split()\n",
    "                    wav_folder = data[3][:-5]\n",
    "                    wav_name = data[3] + \".wav\"\n",
    "                    emo = data[4]\n",
    "                    wav_path = os.path.join(sess_autio_root, wav_folder, wav_name)\n",
    "                    wav_data, _ = sf.read(wav_path, dtype=\"int16\")\n",
    "                    # Ignore samples with length < ignore_length\n",
    "                    if len(wav_data) < ignore_length:\n",
    "                        logging.warning(f\"Ignoring sample {wav_path} with length {len(wav_data)}\")\n",
    "                        continue\n",
    "                    emo = iemocap2label.get(emo, None)\n",
    "                    if emo is not None:\n",
    "                        samples.append((wav_path, emo))\n",
    "                        labels.append(emo)\n",
    "\n",
    "# Shuffle and split\n",
    "temp = list(zip(samples, labels))\n",
    "random.Random(0).shuffle(temp)\n",
    "samples, labels = zip(*temp)\n",
    "train_samples, test_samples, _, _ = train_test_split(samples, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Save data\n",
    "os.makedirs(\"IEMOCAP_preprocessed\", exist_ok=True)\n",
    "with open(os.path.join(\"IEMOCAP_preprocessed\", \"train.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(train_samples, f)\n",
    "with open(os.path.join(\"IEMOCAP_preprocessed\", \"test.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(test_samples, f)\n",
    "\n",
    "logging.info(f\"Train samples: {len(train_samples)}\")\n",
    "logging.info(f\"Test samples: {len(test_samples)}\")\n",
    "logging.info(f\"Saved to {'IEMOCAP_preprocessed'}\")\n",
    "logging.info(\"Preprocessing finished successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(ABC):\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    @abstractmethod\n",
    "    def show(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "class BaseConfig(Base):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BaseConfig, self).__init__(**kwargs)\n",
    "\n",
    "    def show(self):\n",
    "        for key, value in self.__dict__.items():\n",
    "            logging.info(f\"{key}: {value}\")\n",
    "\n",
    "    def save(self, opt: str):\n",
    "        message = \"\\n\"\n",
    "        for k, v in sorted(vars(opt).items()):\n",
    "            message += f\"{str(k):>30}: {str(v):<40}\\n\"\n",
    "\n",
    "        os.makedirs(os.path.join(opt.checkpoint_dir), exist_ok=True)\n",
    "        out_opt = os.path.join(opt.checkpoint_dir, \"opt.log\")\n",
    "        with open(out_opt, \"w\") as opt_file:\n",
    "            opt_file.write(message)\n",
    "            opt_file.write(\"\\n\")\n",
    "\n",
    "        logging.info(message)\n",
    "\n",
    "    def load(self, opt_path: str):\n",
    "        def decode_value(value: str):\n",
    "            value = value.strip()\n",
    "            if \".\" in value and value.replace(\".\", \"\").isdigit():\n",
    "                value = float(value)\n",
    "            elif value.isdigit():\n",
    "                value = int(value)\n",
    "            elif value == \"True\":\n",
    "                value = True\n",
    "            elif value == \"False\":\n",
    "                value = False\n",
    "            elif value == \"None\":\n",
    "                value = None\n",
    "            elif value.startswith(\"'\") and value.endswith(\"'\") or value.startswith('\"') and value.endswith('\"'):\n",
    "                value = value[1:-1]\n",
    "            return value\n",
    "\n",
    "        with open(opt_path, \"r\") as f:\n",
    "            data = f.read().split(\"\\n\")\n",
    "            # remove all empty strings\n",
    "            data = list(filter(None, data))\n",
    "            # convert to dict\n",
    "            data_dict = {}\n",
    "            for i in range(len(data)):\n",
    "                key, value = data[i].split(\":\")[0].strip(), data[i].split(\":\")[1].strip()\n",
    "                if value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "                    value = value[1:-1].split(\",\")\n",
    "                    value = [decode_value(x) for x in value]\n",
    "                else:\n",
    "                    value = decode_value(value)\n",
    "\n",
    "                data_dict[key] = value\n",
    "        for key, value in data_dict.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "class Config(BaseConfig):\n",
    "    # Base\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Config, self).__init__(**kwargs)\n",
    "        self.name = \"default\"\n",
    "        self.set_args()\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def set_args(self, **kwargs):\n",
    "        # Training settings\n",
    "        self.num_epochs: int = 250\n",
    "        self.checkpoint_dir: str = \"checkpoints/AudioOnly_v2_notebook\"\n",
    "        self.save_all_states: bool = True\n",
    "        self.save_best_val: bool = True\n",
    "        self.max_to_keep: int = 1\n",
    "        self.save_freq: int = 4000\n",
    "        self.batch_size: int = 1\n",
    "\n",
    "        self.loss_type: str = \"CrossEntropyLoss\"\n",
    "\n",
    "        # Learning rate\n",
    "        self.learning_rate: float = 0.01\n",
    "        self.learning_rate_step_size: int = 20\n",
    "        self.learning_rate_gamma: float = 0.1\n",
    "\n",
    "        # Dataset\n",
    "        self.data_name: str = \"IEMOCAPAudio\"  # [IEMOCAP, ESD, MELD, IEMOCAPAudio]\n",
    "        self.data_root: str = \"IEMOCAP_preprocessed\"  # folder contains train.pkl and test.pkl\n",
    "        \n",
    "        # use for training with batch size > 1\n",
    "        self.audio_max_length: int = 546220\n",
    "\n",
    "        # Model\n",
    "        self.num_classes: int = 3\n",
    "        self.dropout: float = 0.5\n",
    "        self.model_type: str = \"AudioOnly_v2\"  # [MMSERA, AudioOnly, TextOnly, SERVER]\n",
    "        self.audio_encoder_type: str = \"lstm\"  # [vggish, panns, hubert_base, wav2vec2_base, wavlm_base, lstm]\n",
    "        self.audio_encoder_dim: int = 512  # 2048 - panns, 128 - vggish, 768 - hubert_base,wav2vec2_base, 512 - wavlm_base\n",
    "        self.audio_norm_type: str = \"layer_norm\"  # [layer_norm, min_max, None]\n",
    "        self.audio_unfreeze: bool = True\n",
    "\n",
    "        self.fusion_head_output_type: str = \"cls\"  # [cls, mean, max]\n",
    "\n",
    "        self.linear_layer_last_dim: int = 64\n",
    "        \n",
    "        # For LSTM\n",
    "        self.lstm_hidden_size=512 # should be the same as audio_encoder_dim\n",
    "        self.lstm_num_layers=2\n",
    "        \n",
    "        self.name = f\"{self.fusion_head_output_type}_{self.audio_encoder_type}\"\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IEMOCAPAudioDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str = \"path/to/data.pkl\",\n",
    "        audio_max_length: int = 546220,\n",
    "        audio_encoder_type: str = \"vggish\",\n",
    "    ):\n",
    "        \"\"\"Dataset for IEMOCAP\n",
    "\n",
    "        Args:\n",
    "            path (str, optional): Path to data.pkl. Defaults to \"path/to/data.pkl\".\n",
    "            tokenizer (BertTokenizer, optional): Tokenizer for text. Defaults to BertTokenizer.from_pretrained(\"bert-base-uncased\").\n",
    "            audio_max_length (int, optional): The maximum length of audio. Defaults to 546220. None for no padding and truncation.\n",
    "        \"\"\"\n",
    "        super(IEMOCAPAudioDataset, self).__init__()\n",
    "        with open(path, \"rb\") as file:\n",
    "            self.data_list = pickle.load(file)\n",
    "        self.audio_max_length = audio_max_length\n",
    "        self.audio_encoder_type = audio_encoder_type\n",
    "\n",
    "    def __paudio__(self, file_path: int) -> torch.Tensor:\n",
    "        wav_data, sr = sf.read(file_path, dtype=\"int16\")\n",
    "        samples = wav_data / 32768.0  # Convert to [-1.0, +1.0]\n",
    "        if self.audio_max_length is not None and samples.shape[0] < self.audio_max_length:\n",
    "            samples = np.pad(samples, (0, self.audio_max_length - samples.shape[0]), \"constant\")\n",
    "        elif self.audio_max_length is not None:\n",
    "            samples = samples[: self.audio_max_length]\n",
    "\n",
    "        if self.audio_encoder_type == \"vggish\":\n",
    "            samples = waveform_to_examples(samples, sr, return_tensor=False)  # num_samples, 96, 64\n",
    "            samples = np.expand_dims(samples, axis=1)  # num_samples, 1, 96, 64\n",
    "        elif self.audio_encoder_type != \"panns\":\n",
    "            samples = torchaudio.functional.resample(samples, sr, 16000)\n",
    "\n",
    "        return torch.from_numpy(samples.astype(np.float32))\n",
    "\n",
    "    def __plabel__(self, label: int) -> torch.Tensor:\n",
    "        return torch.tensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Dict[str, np.ndarray]:\n",
    "        audio_path, label = self.data_list[index]\n",
    "        samples = self.__paudio__(audio_path)\n",
    "        label = self.__plabel__(label)\n",
    "        return samples, label\n",
    "\n",
    "\n",
    "def build_train_test_dataset(opt: Config):\n",
    "    DATASET_MAP = {\n",
    "        \"IEMOCAPAudio\": IEMOCAPAudioDataset,\n",
    "    }\n",
    "\n",
    "    dataset = DATASET_MAP.get(opt.data_name, None)\n",
    "    if dataset is None:\n",
    "        raise NotImplementedError(\n",
    "            \"Dataset {} is not implemented, list of available datasets: {}\".format(opt.data_name, DATASET_MAP.keys())\n",
    "        )\n",
    "\n",
    "    audio_max_length = opt.audio_max_length\n",
    "    if opt.batch_size == 1:\n",
    "        audio_max_length = None\n",
    "\n",
    "    training_data = dataset(\n",
    "        os.path.join(opt.data_root, \"train.pkl\"),\n",
    "        audio_max_length,\n",
    "        opt.audio_encoder_type,\n",
    "    )\n",
    "    test_data = dataset(\n",
    "        os.path.join(opt.data_root, \"test.pkl\"),\n",
    "        None,\n",
    "        opt.audio_encoder_type,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(training_data, batch_size=opt.batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "    return (train_dataloader, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Layer norm with transpose\"\"\"\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        x = input.transpose(-2, -1)\n",
    "        x = nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        x = x.transpose(-2, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvLayerBlock(nn.Module):\n",
    "    \"\"\"Convolution unit of FeatureExtractor\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        bias: bool,\n",
    "        layer_norm: Optional[nn.Module],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.layer_norm = layer_norm\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        length: Optional[Tensor],\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Shape: ``[batch, in_channels, in_frame]``.\n",
    "            length (Tensor or None, optional): Shape ``[batch, ]``.\n",
    "        Returns:\n",
    "            Tensor: Shape ``[batch, out_channels, out_frames]``.\n",
    "            Optional[Tensor]: Shape ``[batch, ]``.\n",
    "        \"\"\"\n",
    "        x = self.conv(x)\n",
    "        if self.layer_norm is not None:\n",
    "            x = self.layer_norm(x)\n",
    "        x = nn.functional.gelu(x)\n",
    "\n",
    "        if length is not None:\n",
    "            length = torch.div(length - self.kernel_size, self.stride, rounding_mode=\"floor\") + 1\n",
    "            # When input length is 0, the resulting length can be negative. So fix it here.\n",
    "            length = torch.max(torch.zeros_like(length), length)\n",
    "        return x, length\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Extract features from audio\n",
    "\n",
    "    Args:\n",
    "        conv_layers (nn.ModuleList):\n",
    "            convolution layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_layers: nn.ModuleList,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv_layers = conv_layers\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        length: Optional[Tensor],\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor):\n",
    "                Input Tensor representing a batch of audio,\n",
    "                shape: ``[batch, time]``.\n",
    "            length (Tensor or None, optional):\n",
    "                Valid length of each input sample. shape: ``[batch, ]``.\n",
    "\n",
    "        Returns:\n",
    "            Tensor:\n",
    "                The resulting feature, shape: ``[batch, frame, feature]``\n",
    "            Optional[Tensor]:\n",
    "                Valid length of each output sample. shape: ``[batch, ]``.\n",
    "        \"\"\"\n",
    "        if x.ndim != 2:\n",
    "            raise ValueError(f\"Expected the input Tensor to be 2D (batch, time). Found: {list(x.shape)}\")\n",
    "\n",
    "        x = x.unsqueeze(1)  # (batch, channel==1, frame)\n",
    "        for layer in self.conv_layers:\n",
    "            x, length = layer(x, length)  # (batch, feature, frame)\n",
    "        x = x.transpose(1, 2)  # (batch, frame, feature)\n",
    "        return x, length\n",
    "\n",
    "\n",
    "################################################################################\n",
    "def get_feature_extractor(\n",
    "    norm_mode: str,\n",
    "    shapes: List[Tuple[int, int, int]],\n",
    "    bias: bool,\n",
    ") -> FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        norm_mode (str):\n",
    "            Either \"group_norm\" or \"layer_norm\".\n",
    "            If \"group_norm\", then a single normalization is applied\n",
    "            in the first convolution block. Otherwise, all the convolution\n",
    "            blocks will have layer normalization.\n",
    "            This option corresponds to \"extractor_mode\" from fairseq.\n",
    "            Expected values are \"group_norm\" for Base arch, and\n",
    "            \"layer_norm\" for Large arch.\n",
    "        shapes (list of tuple of int):\n",
    "            Configuration of convolution layers. List of convolution configuration,\n",
    "            i.e. ``[(output_channel, kernel_size, stride), ...]``\n",
    "            This option corresponds to \"conv_feature_layers\" from fairseq.\n",
    "            Expected values are\n",
    "            ``[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512, 2, 2)] * 2``\n",
    "            for all the architectures.\n",
    "        bias (bool):\n",
    "            Whether to include bias term to each convolution operation.\n",
    "            This option corresponds to \"conv_bias\" from fairseq.\n",
    "            Expected values are False for Base arch, and True for Large arch.\n",
    "\n",
    "    For wav2vec2-base, use\n",
    "        extractor_mode:  group_norm\n",
    "        extractor_conv_layer_config:  [(512, 10, 5), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 2, 2), (512, 2, 2)]\n",
    "        extractor_conv_bias:  False\n",
    "    \"\"\"\n",
    "    if norm_mode not in [\"group_norm\", \"layer_norm\"]:\n",
    "        raise ValueError(\"Invalid norm mode\")\n",
    "    blocks = []\n",
    "    in_channels = 1\n",
    "    for i, (out_channels, kernel_size, stride) in enumerate(shapes):\n",
    "        normalization = None\n",
    "        if norm_mode == \"group_norm\" and i == 0:\n",
    "            normalization = nn.GroupNorm(\n",
    "                num_groups=out_channels,\n",
    "                num_channels=out_channels,\n",
    "                affine=True,\n",
    "            )\n",
    "        elif norm_mode == \"layer_norm\":\n",
    "            normalization = LayerNorm(\n",
    "                normalized_shape=out_channels,\n",
    "                elementwise_affine=True,\n",
    "            )\n",
    "        blocks.append(\n",
    "            ConvLayerBlock(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                bias=bias,\n",
    "                layer_norm=normalization,\n",
    "            )\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "    return FeatureExtractor(nn.ModuleList(blocks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VGGish(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(VGGish, self).__init__()\n",
    "#         self.vggish = vggish()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = []\n",
    "#         for i in range(x.size(0)):\n",
    "#             out.append(self.vggish(x[i]))\n",
    "#         x = torch.stack(out, axis=0)\n",
    "#         if len(x.size()) == 2:\n",
    "#             x = x.unsqueeze(1)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# def build_vggish_encoder(opt: Config) -> nn.Module:\n",
    "#     \"\"\"A function to build vggish encoder\"\"\"\n",
    "#     return VGGish()\n",
    "\n",
    "# class HuBertBase(nn.Module):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(HuBertBase, self).__init__(**kwargs)\n",
    "#         bundle = torchaudio.pipelines.HUBERT_BASE\n",
    "#         self.model = bundle.get_model()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features, _ = self.model(x)\n",
    "#         return features\n",
    "\n",
    "# def build_hubert_base_encoder(opt: Config) -> nn.Module:\n",
    "#     \"\"\"A function to build hubert encoder\"\"\"\n",
    "#     return HuBertBase()\n",
    "\n",
    "\n",
    "# class Wav2Vec2Base(nn.Module):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(Wav2Vec2Base, self).__init__(**kwargs)\n",
    "#         bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "#         self.model = bundle.get_model()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features, _ = self.model(x)\n",
    "#         return features\n",
    "\n",
    "\n",
    "# def build_wav2vec2_base_encoder(opt: Config) -> nn.Module:\n",
    "#     return Wav2Vec2Base()\n",
    "\n",
    "\n",
    "# class WavlmBase(nn.Module):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(WavlmBase, self).__init__(**kwargs)\n",
    "#         bundle = torchaudio.pipelines.WAVLM_BASE\n",
    "#         self.model = bundle.get_model()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features, _ = self.model(x)\n",
    "#         return features\n",
    "\n",
    "\n",
    "# def build_wavlm_base_encoder(opt: Config) -> nn.Module:\n",
    "#     return WavlmBase()\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, feature_module, input_size=512, hidden_size=512, num_layers=2, **kwargs):\n",
    "        super(LSTM, self).__init__(**kwargs)\n",
    "        self.feature_module = feature_module\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, lengths = self.feature_module(x, None)  # (samples, length)\n",
    "        x, _ = self.lstm(x)\n",
    "        # take only the last output\n",
    "        x = x[:, -1, :]\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_lstm_encoder(opt: Config) -> nn.Module:\n",
    "\n",
    "    extractor_mode = \"group_norm\"\n",
    "    extractor_conv_layer_config = [(512, 10, 5), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 2, 2), (512, 2, 2)]\n",
    "    extractor_conv_bias = False\n",
    "    feature_extractor = get_feature_extractor(extractor_mode, extractor_conv_layer_config, extractor_conv_bias)\n",
    "    feature_extractor.to(\"cpu\")\n",
    "    state_dict = torch.load(os.path.join(\"D:/LSTM/feature_extractor_wav2vec_base.pth\"))\n",
    "    feature_extractor.load_state_dict(state_dict)\n",
    "\n",
    "    model = LSTM(feature_extractor, input_size=512, hidden_size=opt.lstm_hidden_size, num_layers=opt.lstm_num_layers)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_audio_encoder(opt: Config) -> nn.Module:\n",
    "    \"\"\"A function to build audio encoder\n",
    "\n",
    "    Args:\n",
    "        type (str, optional): Type of audio encoder. Defaults to \"vggish\".\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Audio encoder\n",
    "    \"\"\"\n",
    "    type = opt.audio_encoder_type\n",
    "    encoders = {\n",
    "        # \"vggish\": build_vggish_encoder,\n",
    "        # \"hubert_base\": build_hubert_base_encoder,\n",
    "        # \"wav2vec2_base\": build_wav2vec2_base_encoder,\n",
    "        # \"wavlm_base\": build_wavlm_base_encoder,\n",
    "        \"lstm\": build_lstm_encoder,\n",
    "    }\n",
    "    assert type in encoders.keys(), f\"Invalid audio encoder type: {type}\"\n",
    "    return encoders[type](opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioOnly_v2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        opt: Config,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        \"\"\"Speech Emotion Recognition with Audio Only\n",
    "\n",
    "        Args:\n",
    "            opt (Config): Config object\n",
    "            device (str, optional): The device to use. Defaults to \"cpu\".\n",
    "        \"\"\"\n",
    "        super(AudioOnly_v2, self).__init__()\n",
    "\n",
    "        # Audio module\n",
    "        self.audio_encoder = build_audio_encoder(opt)\n",
    "        self.audio_encoder.to(device)\n",
    "        # Freeze/Unfreeze the audio module\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = opt.audio_unfreeze\n",
    "\n",
    "        self.dropout = nn.Dropout(opt.dropout)\n",
    "        self.linear1 = nn.Linear(opt.audio_encoder_dim, 256)\n",
    "        self.linear2 = nn.Linear(256, 64)\n",
    "        self.classifer = nn.Linear(64, opt.num_classes)\n",
    "        self.fusion_head_output_type = opt.fusion_head_output_type\n",
    "\n",
    "    def forward(self, audio: torch.Tensor):\n",
    "        # Audio processing\n",
    "        audio_embeddings = self.audio_encoder(audio)\n",
    "\n",
    "        # Check if vggish outputs is (128) or (num_samples, 128)\n",
    "        if len(audio_embeddings.size()) == 1:\n",
    "            audio_embeddings = audio_embeddings.unsqueeze(0)\n",
    "\n",
    "        # Expand the audio embeddings to match the text embeddings\n",
    "        if len(audio_embeddings.size()) == 2:\n",
    "            audio_embeddings = audio_embeddings.unsqueeze(0)\n",
    "\n",
    "        # Get classification output\n",
    "        if self.fusion_head_output_type == \"cls\":\n",
    "            audio_embeddings = audio_embeddings[:, 0, :]\n",
    "        elif self.fusion_head_output_type == \"mean\":\n",
    "            audio_embeddings = audio_embeddings.mean(dim=1)\n",
    "        elif self.fusion_head_output_type == \"max\":\n",
    "            audio_embeddings = audio_embeddings.max(dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid fusion head output type\")\n",
    "\n",
    "        # Classification head\n",
    "        x = self.linear1(audio_embeddings)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        out = self.classifer(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 11:57:17,635 - root - INFO - Initializing model...\n",
      "2023-10-26 11:57:17,757 - root - INFO - Initializing checkpoint directory and dataset...\n",
      "2023-10-26 11:57:17,761 - root - INFO - \n",
      "             audio_encoder_dim: 512                                     \n",
      "            audio_encoder_type: lstm                                    \n",
      "              audio_max_length: 546220                                  \n",
      "               audio_norm_type: layer_norm                              \n",
      "                audio_unfreeze: True                                    \n",
      "                    batch_size: 1                                       \n",
      "                checkpoint_dir: d:\\LSTM\\checkpoints\\AudioOnly_v2_notebook\\cls_lstm\\20231026-115717\n",
      "                     data_name: IEMOCAPAudio                            \n",
      "                     data_root: IEMOCAP_preprocessed                    \n",
      "                       dropout: 0.5                                     \n",
      "       fusion_head_output_type: cls                                     \n",
      "                 learning_rate: 0.01                                    \n",
      "           learning_rate_gamma: 0.1                                     \n",
      "       learning_rate_step_size: 20                                      \n",
      "         linear_layer_last_dim: 64                                      \n",
      "                     loss_type: CrossEntropyLoss                        \n",
      "              lstm_hidden_size: 512                                     \n",
      "               lstm_num_layers: 2                                       \n",
      "                   max_to_keep: 1                                       \n",
      "                    model_type: AudioOnly_v2                            \n",
      "                          name: cls_lstm                                \n",
      "                   num_classes: 3                                       \n",
      "                    num_epochs: 250                                     \n",
      "               save_all_states: True                                    \n",
      "                 save_best_val: True                                    \n",
      "                     save_freq: 4000                                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt = Config()\n",
    "\n",
    "logging.info(\"Initializing model...\")\n",
    "model = AudioOnly_v2(opt)\n",
    "model.to(device)\n",
    "\n",
    "logging.info(\"Initializing checkpoint directory and dataset...\")\n",
    "# Preapre the checkpoint directory\n",
    "opt.checkpoint_dir = checkpoint_dir = os.path.join(\n",
    "    os.path.abspath(opt.checkpoint_dir),\n",
    "    opt.name,\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    ")\n",
    "weight_dir = os.path.join(checkpoint_dir, \"weights\")\n",
    "os.makedirs(weight_dir, exist_ok=True)\n",
    "opt.save(opt)\n",
    "weight_path = os.path.join(weight_dir, \"latest_ckpt.pth\")\n",
    "weight_best_path = os.path.join(weight_dir, \"best_ckpt.pth\")\n",
    "\n",
    "# Build dataset\n",
    "train_ds, test_ds = build_train_test_dataset(opt)\n",
    "\n",
    "# Build optimizer and criterion\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=opt.learning_rate)\n",
    "lr_scheduler = None\n",
    "if opt.learning_rate_step_size is not None:\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=opt.learning_rate_step_size, gamma=opt.learning_rate_gamma\n",
    "    )\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 11:57:43,989 - root - INFO - Start training...\n",
      "Loss: 1.2081342935562134, Acc: 0.0:   0%|          | 4/6023 [00:03<1:23:18,  1.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\LSTM\\train copy.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/LSTM/train%20copy.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m out \u001b[39m=\u001b[39m model(audio)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/LSTM/train%20copy.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(out, label)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/LSTM/train%20copy.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/LSTM/train%20copy.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/LSTM/train%20copy.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m lr_scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.info(\"Start training...\")\n",
    "step = 0\n",
    "best_acc = -999999\n",
    "for epoch in range(1, opt.num_epochs+1):\n",
    "    with tqdm(total=len(train_ds), ascii=True) as pbar:\n",
    "        pbar.update(1)\n",
    "        for batch in train_ds:\n",
    "            audio, label = batch\n",
    "            # Training step\n",
    "            step += 1\n",
    "            audio = audio.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(audio)\n",
    "            loss = loss_fn(out, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "            if step % opt.save_freq == 0:\n",
    "                torch.save(model.state_dict(), weight_path)\n",
    "                logging.info(\"Saved checkpoint to {} at iter {}, epoch {}\".format(weight_path, step, epoch))\n",
    "                \n",
    "            acc = (out.argmax(dim=1) == label).float().mean()\n",
    "            # Add logs, update progress bar\n",
    "            postfix = \"Loss: {}, Acc: {}\".format(loss.item(), acc.item())\n",
    "            pbar.set_description(postfix)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    for (audio, label) in tqdm(test_ds):\n",
    "        audio = audio.to(device)\n",
    "        label = label.to(device)\n",
    "        out = model(audio)\n",
    "        loss = loss_fn(out, label)\n",
    "        acc = (out.argmax(dim=1) == label).float().mean()\n",
    "        val_loss.append(loss.item())\n",
    "        val_acc.append(acc.item())\n",
    "    print(\"Val loss: {}, Val acc: {}\".format(np.mean(val_loss), np.mean(val_acc)))\n",
    "    if np.mean(val_acc) > best_acc:\n",
    "        print(\"Model improved from {} to {}\".format(best_acc, np.mean(val_acc)))\n",
    "        best_acc = np.mean(val_acc)\n",
    "        torch.save(model.state_dict(), weight_best_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3m-ser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
