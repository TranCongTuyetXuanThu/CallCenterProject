{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6965968,"sourceType":"datasetVersion","datasetId":4002001},{"sourceId":6967817,"sourceType":"datasetVersion","datasetId":3927882},{"sourceId":6972460,"sourceType":"datasetVersion","datasetId":3996025}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-16T00:35:30.627806Z","iopub.execute_input":"2023-11-16T00:35:30.628219Z","iopub.status.idle":"2023-11-16T00:35:31.280075Z","shell.execute_reply.started":"2023-11-16T00:35:30.628186Z","shell.execute_reply":"2023-11-16T00:35:31.279112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import subprocess, os, csv\nfrom torch import nn\nimport torch, torchaudio, statistics\nfrom pydub import AudioSegment\nfrom pydub.silence import detect_nonsilent\nimport pandas as pd\nimport warnings, shutil\nimport datetime\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:35:31.281870Z","iopub.execute_input":"2023-11-16T00:35:31.282666Z","iopub.status.idle":"2023-11-16T00:35:36.457619Z","shell.execute_reply.started":"2023-11-16T00:35:31.282627Z","shell.execute_reply":"2023-11-16T00:35:36.456352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# suppress all warnings\nwarnings.filterwarnings(\"ignore\")\n# suppress only the UserWarning with message \"At least one mel filterbank has all zero values.\"\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:35:36.458921Z","iopub.execute_input":"2023-11-16T00:35:36.459402Z","iopub.status.idle":"2023-11-16T00:35:36.465885Z","shell.execute_reply.started":"2023-11-16T00:35:36.459370Z","shell.execute_reply":"2023-11-16T00:35:36.464633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model flow\nclass define_model(nn.Module):\n\n    # Define layers\n    def __init__(self, num_emotions):\n        super().__init__()\n\n        transformer_layer = nn.TransformerEncoderLayer(\n            d_model=40, #####################\n            nhead=4,\n            dim_feedforward=512,\n            dropout=0.4,\n            activation='relu'\n        )\n        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n\n        #maxpool: reshape (width, height)\n        #conv: reshape (channel)\n        conv2d_layer = nn.Sequential(\n            nn.Conv2d(\n                in_channels=2,\n                out_channels=16,\n                kernel_size=3,\n                stride=1,\n                padding=1\n            ),\n            nn.BatchNorm2d(16),#######\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=0.3),\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=3,\n                stride=1,\n                padding=1\n            ),\n            nn.BatchNorm2d(32),#######\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=4),\n            nn.Dropout(p=0.3),\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=64,\n                kernel_size=3,\n                stride=1,\n                padding=1\n            ),\n            nn.BatchNorm2d(64),#######\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=4),\n            nn.Dropout(p=0.3),\n        )\n        self.conv2Dblock1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=2,\n                out_channels=16,\n                kernel_size=3,\n                stride=1,\n                padding=1\n            ),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=0.3),\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=3,\n                stride=1,\n                padding=1\n            ),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=4),\n            nn.Dropout(p=0.3),\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=64,\n                kernel_size=3,\n                stride=1,\n                padding=1\n            ),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=4),\n            nn.Dropout(p=0.3),\n        )\n        self.conv2Dblock2 = conv2d_layer\n\n        self.fc1_layer = nn.Linear(1792*2+40, 1800)\n        self.act1 = nn.ReLU()\n#         self.fc2_layer = nn.Linear(1800, 9)\n#         self.act1 = nn.ReLU()\n        self.fc2_layer = nn.Linear(1800, num_emotions)\n        self.softmax_out = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        conv2d_embedding1 = self.conv2Dblock1(x)\n        conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim = 1)\n\n        conv2d_embedding2 = self.conv2Dblock2(x)\n        conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim = 1)\n\n        x_maxpool = self.transformer_maxpool(x)\n        # x_maxpool_reduced = torch.squeeze(x_maxpool,1) \n        x_maxpool_reduced = x_maxpool.resize(x_maxpool.shape[0], 40, 450) \n        x = x_maxpool_reduced.permute(2,0,1) #rearrange dims\n        transformer_output = self.transformer_encoder(x)\n        transformer_embedding = torch.mean(transformer_output, dim = 0)\n\n        complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2, transformer_embedding], dim = 1)\n        fc1 = self.fc1_layer(complete_embedding)\n        ac1 = self.act1(fc1)\n#         fc2 = self.fc2_layer(ac1)\n#         ac2 = self.act1(fc2)\n        output_logits = self.fc2_layer(ac1)\n        output_softmax = self.softmax_out(output_logits)\n        return output_logits, output_softmax","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:35:36.469518Z","iopub.execute_input":"2023-11-16T00:35:36.470000Z","iopub.status.idle":"2023-11-16T00:35:36.496359Z","shell.execute_reply.started":"2023-11-16T00:35:36.469960Z","shell.execute_reply":"2023-11-16T00:35:36.494932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class preprocess_real_data():\n    def __init__(self,file):\n      self.file = file\n    def mp4_to_wav(self, file):\n        output_file = file.replace('input/real-data/Vietnamese/raw', 'working/wav_datas')\n        output_file = output_file.split('.')[0] + \".wav\"\n        subprocess.call(['ffmpeg', '-i', file, output_file])\n        return output_file\n    def remove_noise(self, file):\n        # Detect non-silent parts of the audio\n        sound_file = AudioSegment.from_wav(file)\n        non_sil_times = detect_nonsilent(sound_file, min_silence_len=400, silence_thresh=sound_file.dBFS * 0.65)\n        # Concatenate the non-silent parts of the audio\n        if len(non_sil_times) > 0:\n            non_sil_times_concat = [non_sil_times[0]]\n            if len(non_sil_times) > 1:\n                for t in non_sil_times[1:]:\n                    if t[0] - non_sil_times_concat[-1][1] < 100:\n                        non_sil_times_concat[-1] = (non_sil_times_concat[-1][0], t[1])\n                    else:\n                        non_sil_times_concat.append(t)\n            new_audio = sound_file[non_sil_times_concat[0][0]:non_sil_times_concat[0][1]]\n            for t in non_sil_times_concat[1:]:\n                new_audio += sound_file[t[0]:t[1]]\n        else:\n            new_audio = sound_file\n\n        # Export the new audio file\n        file_name = file.replace('wav_datas', 'denoised_datas')\n        file_name = file_name[:-4]+'_denoised.wav'\n        new_audio.export(file_name, format=\"wav\")\n        return file_name\n    def resize(self, file):\n        tensor = torch.load(file)\n        n = tensor.shape[-1]//900 + 1\n        sample = torch.zeros((2,40,n * 900))\n        sample[:,:,:tensor.shape[-1]] = tensor\n        sample = sample.reshape((2,40,900,n)).permute(3,0,1,2)\n        torch.save(sample,file)\n#         sample = torch.transpose(torch.transpose(sample1.reshape((40,500,n)),dim0=0, dim1=2),dim0=1, dim1=2).reshape((n,1,40,500))\n        return sample\n    def save_tensor_file(self, file):\n        waveform, sample_rate = torchaudio.load(file)\n        transform = torchaudio.transforms.MFCC(sample_rate=sample_rate)\n        mfcc = transform(waveform)\n        file_name = file[:-4]+'.pt'\n        torch.save(mfcc,file_name)\n        return file_name\n    def complete_preprocessing(self, file):\n      file = self.mp4_to_wav(file)\n      file = self.remove_noise(file)\n      file = self.save_tensor_file(file)\n      tensor = self.resize(file)\n      return file, tensor","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:36:41.084009Z","iopub.execute_input":"2023-11-16T00:36:41.084528Z","iopub.status.idle":"2023-11-16T00:36:41.110061Z","shell.execute_reply.started":"2023-11-16T00:36:41.084481Z","shell.execute_reply":"2023-11-16T00:36:41.108740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(model_path):\n    model = define_model(3)\n    model.eval()\n    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n#     model.load_state_dict(torch.load(model_path))\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:36:41.345828Z","iopub.execute_input":"2023-11-16T00:36:41.346701Z","iopub.status.idle":"2023-11-16T00:36:41.353845Z","shell.execute_reply.started":"2023-11-16T00:36:41.346655Z","shell.execute_reply":"2023-11-16T00:36:41.352532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = load_model('/kaggle/input/dpl-project/model93.pt')\n# # model = torch.load('/kaggle/input/dpl-project/model_update1.pt', map_location='cpu')\n# # model.load_state_dict(torch.load('model.pt'))\n# # model.parameters()\n# num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n# print(f'Total number of trainable parameters: {num_params}')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:36:41.746779Z","iopub.execute_input":"2023-11-16T00:36:41.747157Z","iopub.status.idle":"2023-11-16T00:36:41.752884Z","shell.execute_reply.started":"2023-11-16T00:36:41.747129Z","shell.execute_reply":"2023-11-16T00:36:41.751629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\n\ndef get_wav_duration(file_path):\n    with wave.open(file_path, 'rb') as wav_file:\n        # Get the frame rate (samples per second) and total number of frames\n        frame_rate = wav_file.getframerate()\n        total_frames = wav_file.getnframes()\n\n        # Calculate the duration in seconds\n        duration = total_frames / float(frame_rate)\n    \n    return duration","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:36:41.980086Z","iopub.execute_input":"2023-11-16T00:36:41.980749Z","iopub.status.idle":"2023-11-16T00:36:41.987240Z","shell.execute_reply.started":"2023-11-16T00:36:41.980716Z","shell.execute_reply":"2023-11-16T00:36:41.986089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_real_time(file_sample, model, preprocesser):\n    _, tensor = preprocesser.complete_preprocessing(file_sample)\n    output_logit, output_softmax = model(tensor)\n    output_softmax = torch.argmax(output_softmax, dim=1)\n    emotion_dict = {0: 'positive',\n                    1: 'neutral',\n                    2: 'negative'}\n    labels = [emotion_dict[int(value)] for value in output_softmax]\n    \n    final_output = max(set(output_softmax.tolist()), key=output_softmax.tolist().count)\n    label = emotion_dict[final_output]\n    \n    duration_seconds = get_wav_duration(file_sample)\n    range_ = duration_seconds/ len(labels)\n    a = datetime.time(0, 0)\n    list_labels = list()\n    delta = datetime.timedelta(seconds=round(range_, 5))\n    for label in labels:\n        new_time = datetime.datetime.combine(datetime.datetime.today(), a) + delta\n        list_labels.append(str(a)[3:8]+'-'+str(new_time.time())[3:8]+'_'+str(label))\n        a = new_time.time()\n    return(list_labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:36:42.388412Z","iopub.execute_input":"2023-11-16T00:36:42.388794Z","iopub.status.idle":"2023-11-16T00:36:42.399470Z","shell.execute_reply.started":"2023-11-16T00:36:42.388763Z","shell.execute_reply":"2023-11-16T00:36:42.398054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def waveform(path):\n    waveform, sr = torchaudio.load(path)\n    plt.figure(figsize = (10,4))\n    plt.plot(waveform.numpy().T)\n    plt.title('Waveform')\n    plt.xlabel('Time')\n    plt.ylabel('Frequency')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:36:42.753663Z","iopub.execute_input":"2023-11-16T00:36:42.754355Z","iopub.status.idle":"2023-11-16T00:36:42.760965Z","shell.execute_reply.started":"2023-11-16T00:36:42.754319Z","shell.execute_reply":"2023-11-16T00:36:42.759665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocesser = preprocess_real_data(0)\n# model = load_model('/kaggle/input/models/final.pt')\n# file_sample = '/kaggle/input/dpl-project/test_data/raw_datas/negative/Nhat_01.wav'\n# waveform(file_sample)\n# predict_real_time(file_sample, model, preprocesser)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:36:42.969121Z","iopub.execute_input":"2023-11-16T00:36:42.969980Z","iopub.status.idle":"2023-11-16T00:36:42.975208Z","shell.execute_reply.started":"2023-11-16T00:36:42.969943Z","shell.execute_reply":"2023-11-16T00:36:42.973956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_1_sample(file_sample, model):\n    tensor = torch.load(file_sample)\n    output_logit, output_softmax = model(tensor)\n    output_softmax = torch.argmax(output_softmax, dim=1)\n    final_output = max(set(output_softmax.tolist()), key=output_softmax.tolist().count)\n    emotion_dict = {0: 'positive',\n                    1: 'neutrual',\n                    2: 'negative'}\n    label = emotion_dict[final_output]\n    return final_output, label","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:50:30.656070Z","iopub.execute_input":"2023-11-16T00:50:30.656470Z","iopub.status.idle":"2023-11-16T00:50:30.664347Z","shell.execute_reply.started":"2023-11-16T00:50:30.656441Z","shell.execute_reply":"2023-11-16T00:50:30.663013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_label_file(root):\n  header = ['sessionID','labels','path']\n  emotion_dict = {\n      'positive': 0,\n      'neutrual': 1,\n      'negative': 2\n  }\n  with open('test_real_data.csv',mode='w',newline='') as file:\n          write = csv.writer(file)\n          write.writerow(header)\n  df = pd.read_csv('test_real_data.csv')\n  for file in os.listdir(root):\n          for sample in os.listdir(os.path.join(root, file)):\n              processer = preprocess_real_data(0)\n              path_sample, tensor = processer.complete_preprocessing(os.path.join(root, file, sample))\n              sample_info = {'sessionID': sample, 'labels': emotion_dict[file],'path': path_sample}\n              df = pd.concat([df, pd.DataFrame([sample_info])], ignore_index=True)\n  df.to_csv('test_real_data.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:50:31.313504Z","iopub.execute_input":"2023-11-16T00:50:31.313913Z","iopub.status.idle":"2023-11-16T00:50:31.323262Z","shell.execute_reply.started":"2023-11-16T00:50:31.313882Z","shell.execute_reply":"2023-11-16T00:50:31.322077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_real_data(model_file):\n  df = pd.read_csv('/kaggle/working/test_real_data.csv')\n  df = df.sort_values('labels')\n  model = load_model(model_file)\n  preprocesser = preprocess_real_data('ok')\n  y = df.labels\n  y = torch.as_tensor(y)\n  y_hat = torch.zeros(len(y))\n  ind_sample = 0\n  for sample in df.path:\n    final_output, label = predict_1_sample(sample, model)\n    y_hat[ind_sample] = final_output\n    ind_sample += 1\n  df.to_csv('/kaggle/working/test_real_data.csv')\n  return y, y_hat","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:36:44.058881Z","iopub.execute_input":"2023-11-16T00:36:44.059278Z","iopub.status.idle":"2023-11-16T00:36:44.067798Z","shell.execute_reply.started":"2023-11-16T00:36:44.059248Z","shell.execute_reply":"2023-11-16T00:36:44.066128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shutil.rmtree('/kaggle/working/wav_datas')\n# shutil.rmtree('/kaggle/working/denoised_datas')\n# folder_root = ['denoised_datas','wav_datas']\n# folder = ['positive','neutrual','negative']\n# for fold in folder_root:\n#     os.makedirs(os.path.join('/kaggle/working/',fold))\n#     for fold_ in folder:\n#         os.makedirs(os.path.join('/kaggle/working/',fold,fold_))","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:44:54.550640Z","iopub.execute_input":"2023-11-16T00:44:54.551074Z","iopub.status.idle":"2023-11-16T00:44:54.558740Z","shell.execute_reply.started":"2023-11-16T00:44:54.551025Z","shell.execute_reply":"2023-11-16T00:44:54.557413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root = '/kaggle/input/real-data/Vietnamese/raw'\nextract_label_file(root)\nmodel_file = '/kaggle/input/models/final.pt'\ny, y_hat = test_real_data(model_file)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-16T00:50:35.846099Z","iopub.execute_input":"2023-11-16T00:50:35.847007Z","iopub.status.idle":"2023-11-16T00:55:18.285293Z","shell.execute_reply.started":"2023-11-16T00:50:35.846965Z","shell.execute_reply":"2023-11-16T00:55:18.283833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def metric(y, y_hat):\n  df = pd.read_csv('/kaggle/working/test_real_data.csv')\n  labels = {'positive':0,'neutral':0,'negative':0}\n  start_num = 0\n  for i, label in enumerate(labels):\n    end_num = start_num+len(df.loc[df.labels == i])\n    TP = torch.sum(y[start_num:end_num] == y_hat[start_num:end_num])\n    the_last = torch.cat((y_hat[:start_num],y_hat[end_num:]))\n    FP = torch.count_nonzero( the_last == i)\n    FN = torch.count_nonzero( y_hat[start_num:end_num] != i)\n#     print('Precision of',label, ': ', TP/(TP+FP))\n    F1 = 2*TP / (2*TP+FP+FN)\n    labels[label] = round(F1.item(),2)\n    start_num = end_num\n  accuracy = torch.sum(y==y_hat)/float(len(y))\n  print('F1_score of every label: ', labels)\n  print('Average F1: ',round(statistics.mean(labels.values()),2))\n  print('Accuracy: ', accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:59:35.569204Z","iopub.execute_input":"2023-11-16T00:59:35.569845Z","iopub.status.idle":"2023-11-16T00:59:35.580893Z","shell.execute_reply.started":"2023-11-16T00:59:35.569802Z","shell.execute_reply":"2023-11-16T00:59:35.579678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric(y, y_hat)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:59:38.243271Z","iopub.execute_input":"2023-11-16T00:59:38.243744Z","iopub.status.idle":"2023-11-16T00:59:38.279057Z","shell.execute_reply.started":"2023-11-16T00:59:38.243702Z","shell.execute_reply":"2023-11-16T00:59:38.277866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ncm = confusion_matrix(y, y_hat)\nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:59:43.862059Z","iopub.execute_input":"2023-11-16T00:59:43.863236Z","iopub.status.idle":"2023-11-16T00:59:44.887470Z","shell.execute_reply.started":"2023-11-16T00:59:43.863187Z","shell.execute_reply":"2023-11-16T00:59:44.886342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmn = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n\nax = plt.subplots(figsize=(8, 5.5))[1]\nsns.heatmap(cmn, cmap='flare', annot=True, square=True, linecolor='black', linewidths=0.75, ax = ax, fmt = '.2f', annot_kws={'size': 16})\nax.set_xlabel('Predicted', fontsize=18, fontweight='bold')\nax.xaxis.set_label_position('bottom')\nax.xaxis.set_ticklabels([\"positive\", \"neutral\", \"negative\"], fontsize=16)\nax.set_ylabel('Ground Truth', fontsize=18, fontweight='bold')\nax.yaxis.set_ticklabels([\"positive\", \"neutral\", \"negative\"], fontsize=16)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T00:59:54.575534Z","iopub.execute_input":"2023-11-16T00:59:54.575949Z","iopub.status.idle":"2023-11-16T00:59:55.054596Z","shell.execute_reply.started":"2023-11-16T00:59:54.575915Z","shell.execute_reply":"2023-11-16T00:59:55.053379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](http://)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}