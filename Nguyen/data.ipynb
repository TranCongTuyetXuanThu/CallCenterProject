{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to-do list\n",
    "1) extract feature\n",
    "2) label ([exc, hap, sur] 21%,[neu, fru] 42%,[fea, ang, sad, dis] 38%) -> 1 file.csv\n",
    "3) process dims (double), bias (remove)\n",
    "- add noise\n",
    "- process real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchaudio\n",
    "# !pip install torch\n",
    "# !pip install soundfile\n",
    "#!pip install PySoundFile\n",
    "# print(torchaudio.__version__)\n",
    "# print(torch.__version__)\n",
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from io import StringIO\n",
    "import torch\n",
    "import sndhdr\n",
    "import soundfile\n",
    "import numpy\n",
    "import csv,glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) EXTRACT FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Đọc link file\n",
    "def read_link_wav(type = ('sentences', 'dialog'), sessions = [1,2,3,4,5]):\n",
    "    link = os.path.join('IEMOCAP_full_release', 'Session'+str(sessions), type, 'wav')\n",
    "    links = {}\n",
    "    if type == 'dialog':\n",
    "        return\n",
    "    for dialog in os.listdir(link):\n",
    "        list_sentence = []\n",
    "        for sentence in os.listdir(os.path.join(link,dialog)):\n",
    "            sentence_link = os.path.join(link, dialog, sentence)\n",
    "            list_sentence.append(sentence_link)\n",
    "        links[dialog] = list_sentence\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#.wav to .pt\n",
    "def read_and_save_mfcc(types = ('dialog','sentences')):\n",
    "    for session in range(1,6):\n",
    "        links = read_link_wav(types, session)\n",
    "        root = os.path.join('IEMOCAP_full_release','Session'+str(session),types)\n",
    "        if os.path.exists(os.path.join(root,'mfcc')):\n",
    "            continue\n",
    "        os.makedirs(os.path.join(root,'mfcc'))\n",
    "        for dia in links.keys():\n",
    "            if os.path.exists(os.path.join(root,'mfcc',dia)):\n",
    "                continue\n",
    "            os.makedirs(os.path.join(root,'mfcc',dia))\n",
    "            for sen in links[dia]:\n",
    "                try:\n",
    "                    if sndhdr.what(sen).filetype == 'wav':\n",
    "                        waveform, sample_rate = torchaudio.load(sen)\n",
    "                        transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)\n",
    "                        mel_specgram = transform(waveform)\n",
    "                        output_file = os.path.join(root,'mfcc',dia,os.path.basename(sen)[:-4]+'.pt')\n",
    "                        torch.save(mel_specgram,output_file)\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract feature .wav to .pt\n",
    "# read_and_save_mfcc('sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Label + remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_shape(file):\n",
    "    mfcc = torch.load(file)\n",
    "    return list(mfcc.shape)\n",
    "def create_label_file():\n",
    "    categories = {'positive': ['exc', 'hap', 'sur'],\n",
    "                'neutral': ['neu','fru'],\n",
    "                'negative': ['fea','ang','sad','dis']}\n",
    "    header = ['sessionID','labels','positive','neutral','negative','dims','time_shape','path']\n",
    "    with open('processed_label_data.csv',mode='w',newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "    for session in range(1,6):\n",
    "        root = os.path.join('IEMOCAP_full_release','Session'+str(session),'dialog','EmoEvaluation','*')\n",
    "        files = glob.glob(root)\n",
    "        for file in files:\n",
    "            if file[-3:] =='txt': \n",
    "                with open(file, 'r') as lines:\n",
    "                    for line in lines:\n",
    "                        words = line.split(\"\\t\")\n",
    "                        update_sample(categories,session,words)\n",
    "def update_sample(categories,session,words):\n",
    "    if len(words)==4:\n",
    "        path_file = os.path.join('IEMOCAP_full_release','Session'+str(session),'sentences','mfcc',words[1][:-5],words[1]+'.pt')\n",
    "        if os.path.exists(path_file):\n",
    "            shape = get_shape(path_file)\n",
    "            with open('processed_label_data.csv',mode = 'a',newline = '') as f:\n",
    "                write = csv.writer(f)\n",
    "                if (words[2]) in categories['positive']:\n",
    "                    write.writerow([words[1],words[2],1,0,0,shape,shape[2:],path_file])\n",
    "                if (words[2]) in categories['neutral']:\n",
    "                    write.writerow([words[1],words[2],0,1,0,shape,shape[2:],path_file])\n",
    "                if (words[2]) in categories['negative']:\n",
    "                    write.writerow([words[1],words[2],0,0,1,shape,shape[2:],path_file])\n",
    "                if (words[2]) in ['xxx','oth']:\n",
    "                    os.remove(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_label_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for mono file audio or not\n",
    "def check_mono():\n",
    "    for session in range(1,6):\n",
    "        root = os.path.join('IEMOCAP_full_release','Session'+str(session),'sentences','mfcc')\n",
    "        for dialog in os.listdir(root):\n",
    "            for sentence in os.listdir(os.path.join(root,dialog)):\n",
    "                sample = torch.load(os.path.join(root,dialog,sentence))\n",
    "                if sample.shape[0]!=1:\n",
    "                    return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reform_file_data():\n",
    "    df = pd.read_csv('processed_label_data.csv')\n",
    "    df['dims'] = df['dims'].apply(lambda x: [int(i) for i in x.strip('[]').split(',')])\n",
    "    if str(df['time_shape'].dtype) != 'int64':\n",
    "        df['time_shape'] = df['time_shape'].apply(lambda x: int(x.strip('[]')))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to size(128,1500)\n",
    "def normalize_shape():\n",
    "    df = reform_file_data()\n",
    "    for sample in range(len(df)):\n",
    "        df.time_shape[sample] = 1500\n",
    "        df.dims[sample] = list(df.dims[sample][1],1500)\n",
    "        tmp_sample = torch.load(df.path[sample])\n",
    "        if list(tmp_sample.shape) == [128,1500]:\n",
    "            continue\n",
    "        tmp_sample = tmp_sample.resize(128,tmp_sample.shape[2])\n",
    "        if df.time_shape[sample] >1500:\n",
    "            tmp_sample = tmp_sample[:,:1500]\n",
    "        while tmp_sample.shape[1]<1500:\n",
    "            last_dims = 1500 - tmp_sample.shape[1]\n",
    "            if last_dims > tmp_sample.shape[1]:\n",
    "                tmp_sample = torch.cat((tmp_sample,tmp_sample),dim=1)\n",
    "            else:\n",
    "                tmp_sample = torch.cat((tmp_sample,tmp_sample[:,:last_dims]),dim=1)\n",
    "        torch.save(tmp_sample,df.path[sample])\n",
    "    df.to_csv('processed_shape_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_shape():\n",
    "    df = reform_file_data()\n",
    "    for sample in range(len(df)):\n",
    "        tmp_sample = torch.load(df.path[sample])\n",
    "        if list(tmp_sample.shape) != [128,1500]:\n",
    "            print(list(tmp_sample.shape))\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoang\\AppData\\Local\\Temp\\ipykernel_16696\\1886185345.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.time_shape[sample] = 1500\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list expected at most 1 argument, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hoang\\Pojects\\audio2emotion\\data.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hoang/Pojects/audio2emotion/data.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m normalize_shape()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hoang/Pojects/audio2emotion/data.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m check_shape()\n",
      "\u001b[1;32mc:\\Users\\hoang\\Pojects\\audio2emotion\\data.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hoang/Pojects/audio2emotion/data.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hoang/Pojects/audio2emotion/data.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df\u001b[39m.\u001b[39mtime_shape[sample] \u001b[39m=\u001b[39m \u001b[39m1500\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hoang/Pojects/audio2emotion/data.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     df\u001b[39m.\u001b[39mdims[sample] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(df\u001b[39m.\u001b[39;49mdims[sample][\u001b[39m1\u001b[39;49m],\u001b[39m1500\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hoang/Pojects/audio2emotion/data.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     tmp_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(df\u001b[39m.\u001b[39mpath[sample])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hoang/Pojects/audio2emotion/data.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlist\u001b[39m(tmp_sample\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m [\u001b[39m128\u001b[39m,\u001b[39m1500\u001b[39m]:\n",
      "\u001b[1;31mTypeError\u001b[0m: list expected at most 1 argument, got 2"
     ]
    }
   ],
   "source": [
    "normalize_shape()\n",
    "check_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corrNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
